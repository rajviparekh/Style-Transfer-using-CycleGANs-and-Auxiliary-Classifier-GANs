{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec5a8a7-07b3-4c30-905b-cd5048aa7de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import umap.umap_ as umap\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, initializers, Model\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc9ce19-7868-4e25-ac83-b36f568ab5bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb29436-8e96-47b3-8e54-fae478cc2683",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_style_datasets(style_name, training_split, testing_split, storage_directory='Data'):\n",
    "\n",
    "    training_dataset = tfds.load(f\"cycle_gan/{style_name}\", split=training_split, as_supervised=True, shuffle_files=False,\n",
    "                         data_dir=storage_directory, download=True)\n",
    "    testing_dataset = tfds.load(f\"cycle_gan/{style_name}\", split=testing_split, as_supervised=True, shuffle_files=False,\n",
    "                        data_dir=storage_directory, download=True)\n",
    "    return training_dataset, testing_dataset\n",
    "\n",
    "# Load datasets for Ukiyo-e art\n",
    "ukiyoe_datasets_style = \"ukiyoe2photo\"\n",
    "ukiyoe_training_dataset, ukiyoe_testing_dataset = load_style_datasets(ukiyoe_datasets_style, \"trainA\", \"testA\")\n",
    "\n",
    "# Load datasets for ordinary photographs (aligned with Ukiyo-e for contrast)\n",
    "photographs_datasets_style = \"ukiyoe2photo\"\n",
    "ordinary_photos_training_dataset, ordinary_photos_testing_dataset = load_style_datasets(photographs_datasets_style, \"trainB\", \"testB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26bd540-73d4-439b-b3ba-9b4da450de14",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf8c69-7703-41a8-be68-fa1045d2149e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = (128, 128)\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def preprocess_image(image, label=None):\n",
    "    # Normalize the pixel values to range from -1 to 1\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image / 127.5) - 1.0\n",
    "\n",
    "    # Resize the image\n",
    "    image = tf.image.resize(image, IMG_SIZE)\n",
    "\n",
    "    return (image, label) if label is not None else image\n",
    "\n",
    "def prepare_dataset(dataset):\n",
    "    # Apply preprocessing\n",
    "    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Shuffle, batch, and prefetch the dataset\n",
    "    dataset = dataset.shuffle(buffer_size=BUFFER_SIZE)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Preprocessing and preparing the Paintings dataset\n",
    "paintings_training_dataset = prepare_dataset(ukiyoe_training_dataset)\n",
    "paintings_testing_dataset = prepare_dataset(ukiyoe_testing_dataset)\n",
    "\n",
    "# Preprocessing and preparing the Pictures dataset\n",
    "picture_train_dataset = prepare_dataset(ordinary_photos_training_dataset)\n",
    "picture_test_dataset = prepare_dataset(ordinary_photos_testing_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fd6722-bbbe-4703-b274-93b9e43eaf6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_images(dataset, num_images=25):\n",
    "    # Determine the grid size\n",
    "    num_rows = int(math.sqrt(num_images))\n",
    "    num_cols = int(math.ceil(num_images / num_rows))\n",
    "\n",
    "    # Extract data from the first batch\n",
    "    data = next(iter(dataset))\n",
    "\n",
    "    # Check if the batch includes labels\n",
    "    if isinstance(data, tuple):\n",
    "        image_batch, label_batch = data\n",
    "    else:\n",
    "        image_batch = data\n",
    "        label_batch = None\n",
    "\n",
    "    # Randomly select a subset of images (and corresponding labels, if available)\n",
    "    idx = tf.random.shuffle(tf.range(tf.shape(image_batch)[0]))[:num_images]\n",
    "    image_batch = tf.gather(image_batch, idx)\n",
    "    label_batch = tf.gather(label_batch, idx) if label_batch is not None else None\n",
    "\n",
    "    # Normalize image colors to [0, 1]\n",
    "    image_batch = (image_batch + 1) / 2\n",
    "\n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(10, 10))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i >= num_images:\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "        \n",
    "        img = image_batch[i].numpy()  # Convert to numpy array\n",
    "\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c1f419-300b-45ef-b4ce-00390d43c3e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_images(picture_train_dataset, num_images=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a46204-2f25-4df4-862f-66a6538c73d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_images(paintings_training_dataset, num_images=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356b1552-b2c6-43a2-916f-e0f6c9068f66",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4316a625-4f5f-47bd-ac67-e1d7ac7f785e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb7163-4b84-48fc-b028-9345b75c4bde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def residual_block(input_tensor, input_label, filters, block_idx, concat_label=True):\n",
    "    \"\"\"\n",
    "    Residual block for the generator network.\n",
    "    \"\"\"\n",
    "    init_weights = initializers.RandomNormal(stddev=0.02)\n",
    "\n",
    "    x = input_tensor\n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size=3, strides=1, padding='same', kernel_initializer=init_weights, name=f'ResidualBlock_{block_idx+1}_Conv2D_1')(x)\n",
    "    x = InstanceNormalization(axis=-1, name=f'ResidualBlock_{block_idx+1}_InstanceNorm_1')(x)\n",
    "    x = layers.Activation('relu', name=f'ResidualBlock_{block_idx+1}_ReLU_1')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size=3, strides=1, padding='same', kernel_initializer=init_weights, name=f'ResidualBlock_{block_idx+1}_Conv2D_2')(x)\n",
    "    x = InstanceNormalization(axis=-1, name=f'ResidualBlock_{block_idx+1}_InstanceNorm_2')(x)\n",
    "\n",
    "    x = layers.Add(name=f'ResidualBlock_{block_idx+1}_Addition')([x, input_tensor])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02269e1-de2a-4dc0-b828-5530cf2e35dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_generator(input_shape, name, label_shape=(1,), num_styles=2, num_residual_blocks=9, filters=64, concat_labels_resnet=True, concat_labels_upsample=True):\n",
    "    \"\"\"\n",
    "    Builds the generator network for the CycleGAN model.\n",
    "    \"\"\"\n",
    "    init_weights = initializers.RandomNormal(stddev=0.02)\n",
    "\n",
    "    input_image = layers.Input(shape=input_shape, name='ImageInput')\n",
    "\n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size=7, strides=1, padding='same', kernel_initializer=init_weights, name='Conv2D_1')(input_image)\n",
    "    x = InstanceNormalization(axis=-1, name='InstanceNorm_1')(x)\n",
    "    x = layers.Activation('relu', name='ReLU_1')(x)\n",
    "\n",
    "    for i in range(2):\n",
    "        filters *= 2\n",
    "        x = layers.Conv2D(filters, kernel_size=3, strides=2, padding='same', kernel_initializer=init_weights, name=f'Conv2D_Encoder_{i+1}')(x)\n",
    "        x = InstanceNormalization(axis=-1, name=f'InstanceNorm_Encoder_{i+1}')(x)\n",
    "        x = layers.Activation('relu', name=f'ReLU_Encoder_{i+1}')(x)\n",
    "\n",
    "    for i in range(num_residual_blocks):\n",
    "        x = residual_block(x, input_label, filters, i, concat_labels_resnet)\n",
    "\n",
    "    for i in range(2):\n",
    "        filters //= 2\n",
    "        x = layers.Conv2DTranspose(filters, kernel_size=3, strides=2, padding='same', kernel_initializer=init_weights, name=f'Conv2DTranspose_Decoder_{i+1}')(x)\n",
    "        x = InstanceNormalization(axis=-1, name=f'InstanceNorm_Decoder_{i+1}')(x)\n",
    "        x = layers.Activation('relu', name=f'ReLU_Decoder_{i+1}')(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(3, kernel_size=7, strides=1, padding='same', kernel_initializer=init_weights, name='Conv2DTranspose_Output')(x)\n",
    "    x = InstanceNormalization(axis=-1, name='InstanceNorm_Output')(x)\n",
    "    output_tensor = layers.Activation('tanh', name='TanH_Output')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=input_image, outputs=output_tensor, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdec602-1969-44bb-9b26-5820e7f9fb5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_discriminator(input_shape, name, label_shape=(1,), num_styles=2, filters=64):\n",
    "    init_weights = initializers.RandomNormal(stddev=0.02)\n",
    "\n",
    "    input_image = layers.Input(shape=input_shape, name='ImageInput')\n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size=4, strides=2, padding='same', kernel_initializer=init_weights, name='Conv2D_1')(input_image)\n",
    "    x = layers.LeakyReLU(alpha=0.2, name='LeakyReLU_1')(x)\n",
    "\n",
    "    for i in range(3):\n",
    "        filters *= 2\n",
    "        x = layers.Conv2D(filters, kernel_size=4, strides=2, padding='same', kernel_initializer=init_weights, name=f'Conv2D_{i+2}')(x)\n",
    "        x = InstanceNormalization(axis=-1, name=f'InstanceNorm_{i+1}')(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2, name=f'LeakyReLU_{i+2}')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size=4, strides=1, padding='same', kernel_initializer=init_weights, name=f'Conv2D_{i+1+2}')(x)\n",
    "    x = InstanceNormalization(axis=-1, name=f'InstanceNorm_{i+1+1}')(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2, name=f'LeakyReLU_{i+1+2}')(x)\n",
    "    patch_output = layers.Conv2D(1, kernel_size=4, strides=1, padding='same', kernel_initializer=init_weights, name='Patch_Output')(x)\n",
    "\n",
    "\n",
    "    return tf.keras.Model(inputs=input_image, outputs=patch_output, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7009c3c-99fe-44c4-b03d-99a0edd3c932",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CycleGAN(tf.keras.Model):\n",
    "    def __init__(self, generator_a2b, generator_b2a, discriminator_a, discriminator_b, num_styles, name='CycleGAN', lambda_cycle=10.0, lambda_identity=0.5):\n",
    "        super(CycleGAN, self).__init__(name=name)\n",
    "        self.generator_a2b = generator_a2b\n",
    "        self.generator_b2a = generator_b2a\n",
    "        self.discriminator_a = discriminator_a\n",
    "        self.discriminator_b = discriminator_b\n",
    "        self.num_styles = num_styles\n",
    "        self.lambda_cycle = lambda_cycle\n",
    "        self.lambda_identity = lambda_identity\n",
    "\n",
    "    def compile(self, gen_a2b_optimizer, gen_b2a_optimizer, disc_a_optimizer, disc_b_optimizer, gen_loss_fn, disc_loss_fn, cycle_loss_fn, identity_loss_fn):\n",
    "        super(CycleGAN, self).compile()\n",
    "        self.gen_a2b_optimizer = gen_a2b_optimizer\n",
    "        self.gen_b2a_optimizer = gen_b2a_optimizer\n",
    "        self.disc_a_optimizer = disc_a_optimizer\n",
    "        self.disc_b_optimizer = disc_b_optimizer\n",
    "        self.gen_loss_fn = gen_loss_fn\n",
    "        self.disc_loss_fn = disc_loss_fn\n",
    "        self.cycle_loss_fn = cycle_loss_fn\n",
    "        self.identity_loss_fn = identity_loss_fn\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        real_a, (real_b, real_b_labels) = data\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as gen_tape, tf.GradientTape(persistent=True) as disc_tape:\n",
    "            fake_b = self.generator_a2b([real_a, real_b_labels], training=True)\n",
    "            fake_a = self.generator_b2a([real_b, real_b_labels], training=True)\n",
    "\n",
    "            cycled_a = self.generator_b2a([fake_b, real_b_labels], training=True)\n",
    "            cycled_b = self.generator_a2b([fake_a, real_b_labels], training=True)\n",
    "\n",
    "            same_a = self.generator_b2a([real_a, real_b_labels], training=True)\n",
    "            same_b = self.generator_a2b([real_b, real_b_labels], training=True)\n",
    "\n",
    "            disc_real_a = self.discriminator_a([real_a, real_b_labels], training=False)\n",
    "            disc_real_b = self.discriminator_b([real_b, real_b_labels], training=False)\n",
    "            disc_fake_a = self.discriminator_a([fake_a, real_b_labels], training=False)\n",
    "            disc_fake_b = self.discriminator_b([fake_b, real_b_labels], training=False)\n",
    "\n",
    "            gen_a2b_loss = self.gen_loss_fn(disc_fake_b)\n",
    "            gen_b2a_loss = self.gen_loss_fn(disc_fake_a)\n",
    "\n",
    "            disc_real_a = self.discriminator_a([real_a, real_b_labels], training=True)\n",
    "            disc_real_b = self.discriminator_b([real_b, real_b_labels], training=True)\n",
    "            disc_fake_a = self.discriminator_a([fake_a, real_b_labels], training=True)\n",
    "            disc_fake_b = self.discriminator_b([fake_b, real_b_labels], training=True)\n",
    "\n",
    "            disc_a_loss = self.disc_loss_fn(disc_real_a, disc_fake_a)\n",
    "            disc_b_loss = self.disc_loss_fn(disc_real_b, disc_fake_b)\n",
    "\n",
    "            cycle_a_loss = self.cycle_loss_fn(real_a, cycled_a) * self.lambda_cycle\n",
    "            cycle_b_loss = self.cycle_loss_fn(real_b, cycled_b) * self.lambda_cycle\n",
    "\n",
    "            identity_a_loss = self.identity_loss_fn(real_a, same_a) * self.lambda_identity\n",
    "            identity_b_loss = self.identity_loss_fn(real_b, same_b) * self.lambda_identity\n",
    "\n",
    "            total_gen_a2b_loss = gen_a2b_loss + cycle_b_loss + cycle_a_loss + identity_b_loss \n",
    "            total_gen_b2a_loss = gen_b2a_loss + cycle_b_loss + cycle_a_loss + identity_a_loss \n",
    "            total_disc_a_loss = disc_a_loss \n",
    "            total_disc_b_loss = disc_b_loss \n",
    "\n",
    "        gen_a2b_gradients = gen_tape.gradient(total_gen_a2b_loss, self.generator_a2b.trainable_variables)\n",
    "        gen_b2a_gradients = gen_tape.gradient(total_gen_b2a_loss, self.generator_b2a.trainable_variables)\n",
    "        disc_a_gradients = disc_tape.gradient(total_disc_a_loss, self.discriminator_a.trainable_variables)\n",
    "        disc_b_gradients = disc_tape.gradient(total_disc_b_loss, self.discriminator_b.trainable_variables)\n",
    "\n",
    "        self.gen_a2b_optimizer.apply_gradients(zip(gen_a2b_gradients, self.generator_a2b.trainable_variables))\n",
    "        self.gen_b2a_optimizer.apply_gradients(zip(gen_b2a_gradients, self.generator_b2a.trainable_variables))\n",
    "        self.disc_a_optimizer.apply_gradients(zip(disc_a_gradients, self.discriminator_a.trainable_variables))\n",
    "        self.disc_b_optimizer.apply_gradients(zip(disc_b_gradients, self.discriminator_b.trainable_variables))\n",
    "\n",
    "        return (fake_b, real_b_labels), {\n",
    "            \"gen_a2b_loss\": total_gen_a2b_loss,\n",
    "            \"gen_b2a_loss\": total_gen_b2a_loss,\n",
    "            \"disc_a_loss\": total_disc_a_loss,\n",
    "            \"disc_b_loss\": total_disc_b_loss,\n",
    "            \"cycle_a_loss\": cycle_a_loss,\n",
    "            \"cycle_b_loss\": cycle_b_loss,\n",
    "            \"identity_a_loss\": identity_a_loss,\n",
    "            \"identity_b_loss\": identity_b_loss,\n",
    "        }\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        input_image, input_label = inputs\n",
    "        fake_image = self.generator_a2b([input_image, input_label], training=training)\n",
    "        return fake_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d9b797-c109-423d-9320-ecd9d42527f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loss and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d073ea4-0fba-45cc-a8c3-813b56215d74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Binary cross-entropy loss function for the generator\n",
    "def generator_loss(generated_output):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=generated_output, labels=tf.ones_like(generated_output)))\n",
    "\n",
    "# Binary cross-entropy loss function for the discriminator\n",
    "def discriminator_loss(real_output, generated_output):\n",
    "    real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_output, labels=tf.ones_like(real_output)))\n",
    "    generated_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=generated_output, labels=tf.zeros_like(generated_output)))\n",
    "    return real_loss + generated_loss\n",
    "\n",
    "# L1 loss function for cycle consistency loss\n",
    "def cycle_loss(real_image, cycled_image):\n",
    "    return tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "\n",
    "# L1 loss function for identity loss\n",
    "def identity_loss(real_image, same_image):\n",
    "    return tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "\n",
    "# Create a function to make a learning rate schedule that decays exponentially starting from the specified epoch\n",
    "def create_lr_schedule(initial_lr, decay_start_epoch, dataset, decay_rate=0.9, staircase=False):\n",
    "    steps_per_epoch = len(dataset)\n",
    "    decay_steps = decay_start_epoch * steps_per_epoch\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_lr, decay_steps, decay_rate, staircase=staircase,\n",
    "    )\n",
    "    return lr_schedule\n",
    "\n",
    "# Adam optimizer for the generator A2B with learning rate decay\n",
    "gen_a2b_optimizer = tf.keras.optimizers.Adam(learning_rate=create_lr_schedule(0.00075, 100, paintings_training_dataset, decay_rate=0.85), beta_1=0.9)\n",
    "\n",
    "# Adam optimizer for the generator B2A with learning rate decay\n",
    "gen_b2a_optimizer = tf.keras.optimizers.Adam(learning_rate=create_lr_schedule(0.00075, 100, paintings_training_dataset, decay_rate=0.85), beta_1=0.9)\n",
    "\n",
    "# Adam optimizer for the discriminator A with learning rate decay\n",
    "disc_a_optimizer = tf.keras.optimizers.Adam(learning_rate=create_lr_schedule(1e-5, 100, paintings_training_dataset, decay_rate=0.85), beta_1=0.1)\n",
    "\n",
    "# Adam optimizer for the discriminator B with learning rate decay\n",
    "disc_b_optimizer = tf.keras.optimizers.Adam(learning_rate=create_lr_schedule(1e-5, 100, paintings_training_dataset, decay_rate=0.85), beta_1=0.1)\n",
    "\n",
    "# Define the model's input shape and the number of styles\n",
    "input_shape = (128, 128, 3)\n",
    "label_shape = (1,)\n",
    "num_styles = 2\n",
    "\n",
    "# Build the generator and discriminator models\n",
    "generator_a2b = build_generator(input_shape, name='Generator_A2B', label_shape=label_shape, num_styles=num_styles)\n",
    "generator_b2a = build_generator(input_shape, name='Generator_B2A', label_shape=label_shape, num_styles=num_styles)\n",
    "discriminator_a = build_discriminator(input_shape, name='Discriminator_A', label_shape=label_shape, num_styles=num_styles)\n",
    "discriminator_b = build_discriminator(input_shape, name='Discriminator_B', label_shape=label_shape, num_styles=num_styles)\n",
    "\n",
    "# Create the CycleGAN model \n",
    "model = CycleGAN(\n",
    "    generator_a2b, generator_b2a, discriminator_a, discriminator_b, num_styles\n",
    ")\n",
    "\n",
    "# Compile the model with the defined loss functions and optimizers\n",
    "model.compile(\n",
    "    gen_a2b_optimizer, gen_b2a_optimizer, disc_a_optimizer, disc_b_optimizer, generator_loss, discriminator_loss, cycle_loss, identity_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ce88a-1fd1-4d5d-828e-ba8b78b84a30",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f8d3b9-b48c-41dd-9cbb-3ddfb60c31be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "epoch_losses = {\n",
    "                    \"gen_p2a_loss\": [],\n",
    "                    \"gen_a2p_loss\": [],\n",
    "                    \"disc_p_loss\": [],\n",
    "                    \"disc_a_loss\": [],\n",
    "                    \"cycle_p_loss\": [],\n",
    "                    \"cycle_a_loss\": [],\n",
    "                    \"identity_p_loss\": [],\n",
    "                    \"identity_a_loss\": []\n",
    "                }\n",
    "\n",
    "\n",
    "best_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "dataset = tf.data.Dataset.zip((picture_train_dataset, paintings_training_dataset))\n",
    "for epoch in range(start_epoch,max_epochs):\n",
    "\n",
    "    for step, (real_p, (real_a, real_a_labels)) in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "\n",
    "        fake_paintings, loss_dict = model.train_step((real_p, (real_a, real_a_labels)))\n",
    "        for loss_name, loss_value in loss_dict.items():\n",
    "            batch_losses[loss_name].append(loss_value.numpy())\n",
    "\n",
    "    print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef84d5-b22c-4a27-ad0c-9852ec70315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Epoch Losses disctionary\n",
    "with open(parent_directory+folder_name+'/epoch_losses.pickle', 'rb') as f:\n",
    "            epoch_losses = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23206cf4-59bf-4346-8cb5-fa8d2c2d4eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the figure and axis objects for the plot\n",
    "fig,ax = plt.subplots(figsize=(12,7))\n",
    "\n",
    "# Setting the grid properties\n",
    "ax.grid(color='#adadad',axis='both',which='major',linestyle='--',alpha=0.5,zorder=-1)\n",
    "\n",
    "# Setting the properties of the plot border\n",
    "plt.setp(ax.spines.values(), linewidth=2, color='k')\n",
    "\n",
    "epochs = range(1, len(epoch_losses[\"gen_p2a_loss\"]) + 1)\n",
    "\n",
    "# Plot the losses for each epoch\n",
    "plt.plot(epochs, epoch_losses[\"gen_p2a_loss\"], lw=3.5, label=\"Photo to Art Generator Loss\")\n",
    "plt.plot(epochs, epoch_losses[\"gen_a2p_loss\"], lw=3.5, label=\"Art to Photo Generator Loss\")\n",
    "plt.plot(epochs, epoch_losses[\"disc_p_loss\"], lw=3.5, label=\"Photo Discriminator Loss\")\n",
    "plt.plot(epochs, epoch_losses[\"disc_a_loss\"], lw=3.5, label=\"Art Discriminator Loss\")\n",
    "plt.plot(epochs, epoch_losses[\"aux_class_a_loss\"], lw=3.5, label=\"Art Auxiliary Classifier Loss\")\n",
    "plt.plot(epochs, epoch_losses[\"cycle_p_loss\"], lw=3.5, label=\"Photo Cycle Loss\")\n",
    "plt.plot(epochs, epoch_losses[\"cycle_a_loss\"], lw=3.5, label=\"Art Cycle Loss\")\n",
    "plt.plot(epochs, epoch_losses[\"identity_p_loss\"], lw=3.5, label=\"Photo Identity Loss\")\n",
    "plt.plot(epochs, epoch_losses[\"identity_a_loss\"], lw=3.5, label=\"Art Identity Loss\")\n",
    "\n",
    "# Setting the properties of the ticks on both the axes\n",
    "ax.tick_params(axis='both', which='major',labelcolor='k',labelsize=14, length=0)\n",
    "\n",
    "# Setting the labels for the x and y axes\n",
    "ax.set_xlabel('Epochs',fontsize=18)\n",
    "ax.set_ylabel('Losses',fontsize=18)\n",
    "\n",
    "# Plotting the legend\n",
    "legend = ax.legend(handleheight=1,handlelength=2)\n",
    "# Setting up the legend properties\n",
    "legend.set_title('Type of Loss')\n",
    "legend.get_title().set_fontsize(14)\n",
    "legend.get_frame().set_linewidth(2)\n",
    "legend.get_frame().set_edgecolor('k')\n",
    "legend.get_frame().set_facecolor('None')\n",
    "[text.set_fontsize(12) for text in legend.get_texts()]\n",
    "\n",
    "# Setting the title\n",
    "ax.set_title('Loss Plot',fontsize=22)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21e5d3a-42fd-4836-8739-2ebfe37ec260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an AC Cycle GAN model using the class created above\n",
    "model = AuxiliaryClassifierCycleGAN(generator_p2a, generator_a2p, discriminator_p, discriminator_a, num_classes)\n",
    "\n",
    "# Compiling the AC Cycle GAN\n",
    "model.compile(\n",
    "    gen_p2a_optimizer,\n",
    "    gen_a2p_optimizer,\n",
    "    disc_p_optimizer,\n",
    "    disc_a_optimizer,\n",
    "    generator_loss,\n",
    "    discriminator_loss,\n",
    "    auxiliary_classification_loss,\n",
    "    cycle_consistency_loss,\n",
    "    identity_loss,\n",
    ")\n",
    "\n",
    "# Calling the model with some sample input to initialize the variables\n",
    "dummy_input = (tf.zeros([1, 128, 128, 3]), tf.zeros([1, ]))\n",
    "_ = model(dummy_input,training=False)\n",
    "\n",
    "# Loading model weights\n",
    "model.load_weights(parent_directory+folder_name+'/best_generator_p2a_loss.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bff08dc-5f93-4b59-88e3-363311c4a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot a multiple photos alongside translated versions of both art-styles using the AC Cycle GAN model\n",
    "def plot_generated_images(dataset, num_images=6, num_columns=6, separation=True):\n",
    "\n",
    "    # Calculate the number of rows needed based on the number of images and columns\n",
    "    num_rows = (num_images * 3) // num_columns\n",
    "\n",
    "    if (num_images * 3) % num_columns != 0:\n",
    "        num_rows += 1\n",
    "\n",
    "    # Get a batch of images from the dataset\n",
    "    data = next(iter(dataset))\n",
    "\n",
    "    # Randomly select a subset of images from the batch\n",
    "    idx = tf.random.shuffle(tf.range(data.shape[0]))[:num_images]\n",
    "    image_batch = tf.gather(data, idx)\n",
    "\n",
    "    # Generate Ukiyo-e and Monet-style images for the batch\n",
    "    ukiyo_e_images = model.predict((image_batch, tf.ones((image_batch.shape[0]))))\n",
    "    monet_images = model.predict((image_batch, tf.zeros((image_batch.shape[0]))))\n",
    "\n",
    "    # Create a plot with the specified number of rows and columns\n",
    "    fig, axs = plt.subplots(num_rows, num_columns, figsize=(20, 10))\n",
    "\n",
    "    # Loop over each row and column in the plot and add the corresponding image\n",
    "    for row in range(num_rows):\n",
    "        for col in range(num_columns):\n",
    "\n",
    "            index = (row * num_columns + col) // 3\n",
    "            style = (row * num_columns + col) % 3\n",
    "\n",
    "            if col % 3 == 0:\n",
    "                # Original image\n",
    "                axs[row, col].imshow((image_batch[index] + 1) / 2)\n",
    "                if row == 0:\n",
    "                    axs[row, col].set_title('Ordinary Pictures',fontsize=18)\n",
    "            elif col % 3 == 1:\n",
    "                # Ukiyo-e style generated image\n",
    "                axs[row, col].imshow((monet_images[index] + 1) / 2)\n",
    "                if row == 0:\n",
    "                    axs[row, col].set_title('Monet versions',fontsize=18)\n",
    "            elif col % 3 == 2:\n",
    "                # Monet style generated image\n",
    "                axs[row, col].imshow((ukiyo_e_images[index] + 1) / 2)\n",
    "                if row == 0:\n",
    "                    axs[row, col].set_title('Ukiyo-e versions',fontsize=18)\n",
    "\n",
    "            axs[row, col].axis('off')\n",
    "\n",
    "    # Redraw the canvas to update the plot\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    # Add a vertical line between the two subplot groups\n",
    "    if separation:\n",
    "        line = fig.add_artist(plt.Line2D([0.5, 0.5], [0, 0.95], color='k', lw=3, ls='--'))\n",
    "\n",
    "    # Add a title to the plot and adjust the layout\n",
    "    fig.suptitle(\"Original Photographs and Generated Paintings\", y=1, fontsize=28, fontweight='bold')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting the images from the test data\n",
    "plot_generated_images(picture_test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
